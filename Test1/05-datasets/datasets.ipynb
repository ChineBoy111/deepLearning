{
 "cells": [
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-23T09:57:10.424631Z",
     "start_time": "2024-09-23T09:57:10.421159Z"
    }
   },
   "source": [
    "from datasets import *"
   ],
   "outputs": [],
   "execution_count": 3
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# datasets 基本使用"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 加载在线数据集"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-23T09:57:25.091125Z",
     "start_time": "2024-09-23T09:57:12.202078Z"
    }
   },
   "source": [
    "datasets = load_dataset(\"madao33/new-title-chinese\")\n",
    "datasets"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "train.csv:   0%|          | 0.00/22.6M [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "2d665069846d43f9a50276014f62514a"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\wdx\\.conda\\envs\\wdx_torch\\Lib\\site-packages\\huggingface_hub\\file_download.py:147: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\wdx\\.cache\\huggingface\\hub\\datasets--madao33--new-title-chinese. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "dev.csv:   0%|          | 0.00/6.49M [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "35848beaef5c472d83f8ed04d26411a1"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Generating train split:   0%|          | 0/5850 [00:00<?, ? examples/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "f57c321b4a824379a7ad4a1f436aa8cb"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Generating validation split:   0%|          | 0/1679 [00:00<?, ? examples/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "19b73f8b24bc47de87e6072dfed550d3"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['title', 'content'],\n",
       "        num_rows: 5850\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['title', 'content'],\n",
       "        num_rows: 1679\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 4
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 加载数据集合集中的某一项任务"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-23T13:50:35.293383Z",
     "start_time": "2024-09-23T13:50:28.784796Z"
    }
   },
   "source": [
    "boolq_dataset = load_dataset(\"super_glue\", \"boolq\")\n",
    "boolq_dataset"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "super_glue.py:   0%|          | 0.00/30.7k [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "0214fa41d70343a39078beed35f38dcf"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\wdx\\.conda\\envs\\wdx_torch\\Lib\\site-packages\\huggingface_hub\\file_download.py:147: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\wdx\\.cache\\huggingface\\hub\\datasets--super_glue. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "README.md:   0%|          | 0.00/18.2k [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "15b3ded5ea274c65a0d6a3f663905b48"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "ValueError",
     "evalue": "The repository for super_glue contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at https://hf.co/datasets/super_glue.\nPlease pass the argument `trust_remote_code=True` to allow custom code to be run.",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mValueError\u001B[0m                                Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[6], line 1\u001B[0m\n\u001B[1;32m----> 1\u001B[0m boolq_dataset \u001B[38;5;241m=\u001B[39m load_dataset(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124msuper_glue\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mboolq\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m      2\u001B[0m boolq_dataset\n",
      "File \u001B[1;32m~\\.conda\\envs\\wdx_torch\\Lib\\site-packages\\datasets\\load.py:2074\u001B[0m, in \u001B[0;36mload_dataset\u001B[1;34m(path, name, data_dir, data_files, split, cache_dir, features, download_config, download_mode, verification_mode, keep_in_memory, save_infos, revision, token, streaming, num_proc, storage_options, trust_remote_code, **config_kwargs)\u001B[0m\n\u001B[0;32m   2069\u001B[0m verification_mode \u001B[38;5;241m=\u001B[39m VerificationMode(\n\u001B[0;32m   2070\u001B[0m     (verification_mode \u001B[38;5;129;01mor\u001B[39;00m VerificationMode\u001B[38;5;241m.\u001B[39mBASIC_CHECKS) \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m save_infos \u001B[38;5;28;01melse\u001B[39;00m VerificationMode\u001B[38;5;241m.\u001B[39mALL_CHECKS\n\u001B[0;32m   2071\u001B[0m )\n\u001B[0;32m   2073\u001B[0m \u001B[38;5;66;03m# Create a dataset builder\u001B[39;00m\n\u001B[1;32m-> 2074\u001B[0m builder_instance \u001B[38;5;241m=\u001B[39m load_dataset_builder(\n\u001B[0;32m   2075\u001B[0m     path\u001B[38;5;241m=\u001B[39mpath,\n\u001B[0;32m   2076\u001B[0m     name\u001B[38;5;241m=\u001B[39mname,\n\u001B[0;32m   2077\u001B[0m     data_dir\u001B[38;5;241m=\u001B[39mdata_dir,\n\u001B[0;32m   2078\u001B[0m     data_files\u001B[38;5;241m=\u001B[39mdata_files,\n\u001B[0;32m   2079\u001B[0m     cache_dir\u001B[38;5;241m=\u001B[39mcache_dir,\n\u001B[0;32m   2080\u001B[0m     features\u001B[38;5;241m=\u001B[39mfeatures,\n\u001B[0;32m   2081\u001B[0m     download_config\u001B[38;5;241m=\u001B[39mdownload_config,\n\u001B[0;32m   2082\u001B[0m     download_mode\u001B[38;5;241m=\u001B[39mdownload_mode,\n\u001B[0;32m   2083\u001B[0m     revision\u001B[38;5;241m=\u001B[39mrevision,\n\u001B[0;32m   2084\u001B[0m     token\u001B[38;5;241m=\u001B[39mtoken,\n\u001B[0;32m   2085\u001B[0m     storage_options\u001B[38;5;241m=\u001B[39mstorage_options,\n\u001B[0;32m   2086\u001B[0m     trust_remote_code\u001B[38;5;241m=\u001B[39mtrust_remote_code,\n\u001B[0;32m   2087\u001B[0m     _require_default_config_name\u001B[38;5;241m=\u001B[39mname \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m,\n\u001B[0;32m   2088\u001B[0m     \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mconfig_kwargs,\n\u001B[0;32m   2089\u001B[0m )\n\u001B[0;32m   2091\u001B[0m \u001B[38;5;66;03m# Return iterable dataset in case of streaming\u001B[39;00m\n\u001B[0;32m   2092\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m streaming:\n",
      "File \u001B[1;32m~\\.conda\\envs\\wdx_torch\\Lib\\site-packages\\datasets\\load.py:1795\u001B[0m, in \u001B[0;36mload_dataset_builder\u001B[1;34m(path, name, data_dir, data_files, cache_dir, features, download_config, download_mode, revision, token, storage_options, trust_remote_code, _require_default_config_name, **config_kwargs)\u001B[0m\n\u001B[0;32m   1793\u001B[0m     download_config \u001B[38;5;241m=\u001B[39m download_config\u001B[38;5;241m.\u001B[39mcopy() \u001B[38;5;28;01mif\u001B[39;00m download_config \u001B[38;5;28;01melse\u001B[39;00m DownloadConfig()\n\u001B[0;32m   1794\u001B[0m     download_config\u001B[38;5;241m.\u001B[39mstorage_options\u001B[38;5;241m.\u001B[39mupdate(storage_options)\n\u001B[1;32m-> 1795\u001B[0m dataset_module \u001B[38;5;241m=\u001B[39m dataset_module_factory(\n\u001B[0;32m   1796\u001B[0m     path,\n\u001B[0;32m   1797\u001B[0m     revision\u001B[38;5;241m=\u001B[39mrevision,\n\u001B[0;32m   1798\u001B[0m     download_config\u001B[38;5;241m=\u001B[39mdownload_config,\n\u001B[0;32m   1799\u001B[0m     download_mode\u001B[38;5;241m=\u001B[39mdownload_mode,\n\u001B[0;32m   1800\u001B[0m     data_dir\u001B[38;5;241m=\u001B[39mdata_dir,\n\u001B[0;32m   1801\u001B[0m     data_files\u001B[38;5;241m=\u001B[39mdata_files,\n\u001B[0;32m   1802\u001B[0m     cache_dir\u001B[38;5;241m=\u001B[39mcache_dir,\n\u001B[0;32m   1803\u001B[0m     trust_remote_code\u001B[38;5;241m=\u001B[39mtrust_remote_code,\n\u001B[0;32m   1804\u001B[0m     _require_default_config_name\u001B[38;5;241m=\u001B[39m_require_default_config_name,\n\u001B[0;32m   1805\u001B[0m     _require_custom_configs\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mbool\u001B[39m(config_kwargs),\n\u001B[0;32m   1806\u001B[0m )\n\u001B[0;32m   1807\u001B[0m \u001B[38;5;66;03m# Get dataset builder class from the processing script\u001B[39;00m\n\u001B[0;32m   1808\u001B[0m builder_kwargs \u001B[38;5;241m=\u001B[39m dataset_module\u001B[38;5;241m.\u001B[39mbuilder_kwargs\n",
      "File \u001B[1;32m~\\.conda\\envs\\wdx_torch\\Lib\\site-packages\\datasets\\load.py:1671\u001B[0m, in \u001B[0;36mdataset_module_factory\u001B[1;34m(path, revision, download_config, download_mode, dynamic_modules_path, data_dir, data_files, cache_dir, trust_remote_code, _require_default_config_name, _require_custom_configs, **download_kwargs)\u001B[0m\n\u001B[0;32m   1666\u001B[0m                 \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m   1667\u001B[0m                     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mFileNotFoundError\u001B[39;00m(\n\u001B[0;32m   1668\u001B[0m                         \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mCouldn\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mt find any data file at \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mrelative_to_absolute_path(path)\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m. \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m   1669\u001B[0m                         \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mCouldn\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mt find \u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mpath\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m on the Hugging Face Hub either: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28mtype\u001B[39m(e1)\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__name__\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00me1\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m   1670\u001B[0m                     ) \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m-> 1671\u001B[0m             \u001B[38;5;28;01mraise\u001B[39;00m e1 \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[0;32m   1672\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m trust_remote_code:\n\u001B[0;32m   1673\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mFileNotFoundError\u001B[39;00m(\n\u001B[0;32m   1674\u001B[0m         \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mCouldn\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mt find a dataset script at \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mrelative_to_absolute_path(combined_path)\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m or any data file in the same directory.\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m   1675\u001B[0m     )\n",
      "File \u001B[1;32m~\\.conda\\envs\\wdx_torch\\Lib\\site-packages\\datasets\\load.py:1638\u001B[0m, in \u001B[0;36mdataset_module_factory\u001B[1;34m(path, revision, download_config, download_mode, dynamic_modules_path, data_dir, data_files, cache_dir, trust_remote_code, _require_default_config_name, _require_custom_configs, **download_kwargs)\u001B[0m\n\u001B[0;32m   1629\u001B[0m             \u001B[38;5;28;01mpass\u001B[39;00m\n\u001B[0;32m   1630\u001B[0m     \u001B[38;5;66;03m# Otherwise we must use the dataset script if the user trusts it\u001B[39;00m\n\u001B[0;32m   1631\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m HubDatasetModuleFactoryWithScript(\n\u001B[0;32m   1632\u001B[0m         path,\n\u001B[0;32m   1633\u001B[0m         revision\u001B[38;5;241m=\u001B[39mrevision,\n\u001B[0;32m   1634\u001B[0m         download_config\u001B[38;5;241m=\u001B[39mdownload_config,\n\u001B[0;32m   1635\u001B[0m         download_mode\u001B[38;5;241m=\u001B[39mdownload_mode,\n\u001B[0;32m   1636\u001B[0m         dynamic_modules_path\u001B[38;5;241m=\u001B[39mdynamic_modules_path,\n\u001B[0;32m   1637\u001B[0m         trust_remote_code\u001B[38;5;241m=\u001B[39mtrust_remote_code,\n\u001B[1;32m-> 1638\u001B[0m     )\u001B[38;5;241m.\u001B[39mget_module()\n\u001B[0;32m   1639\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m   1640\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m HubDatasetModuleFactoryWithoutScript(\n\u001B[0;32m   1641\u001B[0m         path,\n\u001B[0;32m   1642\u001B[0m         revision\u001B[38;5;241m=\u001B[39mrevision,\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m   1646\u001B[0m         download_mode\u001B[38;5;241m=\u001B[39mdownload_mode,\n\u001B[0;32m   1647\u001B[0m     )\u001B[38;5;241m.\u001B[39mget_module()\n",
      "File \u001B[1;32m~\\.conda\\envs\\wdx_torch\\Lib\\site-packages\\datasets\\load.py:1318\u001B[0m, in \u001B[0;36mHubDatasetModuleFactoryWithScript.get_module\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m   1311\u001B[0m importable_file_path \u001B[38;5;241m=\u001B[39m _get_importable_file_path(\n\u001B[0;32m   1312\u001B[0m     dynamic_modules_path\u001B[38;5;241m=\u001B[39mdynamic_modules_path,\n\u001B[0;32m   1313\u001B[0m     module_namespace\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mdatasets\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[0;32m   1314\u001B[0m     subdirectory_name\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mhash\u001B[39m,\n\u001B[0;32m   1315\u001B[0m     name\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mname,\n\u001B[0;32m   1316\u001B[0m )\n\u001B[0;32m   1317\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m os\u001B[38;5;241m.\u001B[39mpath\u001B[38;5;241m.\u001B[39mexists(importable_file_path):\n\u001B[1;32m-> 1318\u001B[0m     trust_remote_code \u001B[38;5;241m=\u001B[39m resolve_trust_remote_code(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtrust_remote_code, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mname)\n\u001B[0;32m   1319\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m trust_remote_code:\n\u001B[0;32m   1320\u001B[0m         _create_importable_file(\n\u001B[0;32m   1321\u001B[0m             local_path\u001B[38;5;241m=\u001B[39mlocal_path,\n\u001B[0;32m   1322\u001B[0m             local_imports\u001B[38;5;241m=\u001B[39mlocal_imports,\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m   1328\u001B[0m             download_mode\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdownload_mode,\n\u001B[0;32m   1329\u001B[0m         )\n",
      "File \u001B[1;32m~\\.conda\\envs\\wdx_torch\\Lib\\site-packages\\datasets\\load.py:131\u001B[0m, in \u001B[0;36mresolve_trust_remote_code\u001B[1;34m(trust_remote_code, repo_id)\u001B[0m\n\u001B[0;32m    128\u001B[0m         signal\u001B[38;5;241m.\u001B[39malarm(\u001B[38;5;241m0\u001B[39m)\n\u001B[0;32m    129\u001B[0m     \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mException\u001B[39;00m:\n\u001B[0;32m    130\u001B[0m         \u001B[38;5;66;03m# OS which does not support signal.SIGALRM\u001B[39;00m\n\u001B[1;32m--> 131\u001B[0m         \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\n\u001B[0;32m    132\u001B[0m             \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mThe repository for \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mrepo_id\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m contains custom code which must be executed to correctly \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    133\u001B[0m             \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mload the dataset. You can inspect the repository content at https://hf.co/datasets/\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mrepo_id\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m.\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    134\u001B[0m             \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mPlease pass the argument `trust_remote_code=True` to allow custom code to be run.\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    135\u001B[0m         )\n\u001B[0;32m    136\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m    137\u001B[0m     \u001B[38;5;66;03m# For the CI which might put the timeout at 0\u001B[39;00m\n\u001B[0;32m    138\u001B[0m     _raise_timeout_error(\u001B[38;5;28;01mNone\u001B[39;00m, \u001B[38;5;28;01mNone\u001B[39;00m)\n",
      "\u001B[1;31mValueError\u001B[0m: The repository for super_glue contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at https://hf.co/datasets/super_glue.\nPlease pass the argument `trust_remote_code=True` to allow custom code to be run."
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 按照数据集划分进行加载"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset(\"madao33/new-title-chinese\", split=\"train\")\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-23T09:58:47.571593Z",
     "start_time": "2024-09-23T09:58:42.990655Z"
    }
   },
   "source": [
    "dataset = load_dataset(\"madao33/new-title-chinese\", split=\"train[10:100]\")\n",
    "dataset"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['title', 'content'],\n",
       "    num_rows: 90\n",
       "})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 5
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset(\"madao33/new-title-chinese\", split=\"train[:50%]\")\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset(\"madao33/new-title-chinese\", split=[\"train[:50%]\", \"train[50%:]\"])\n",
    "dataset"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 查看数据集"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-23T13:50:57.097260Z",
     "start_time": "2024-09-23T13:50:52.216827Z"
    }
   },
   "source": [
    "datasets = load_dataset(\"madao33/new-title-chinese\")\n",
    "datasets"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['title', 'content'],\n",
       "        num_rows: 5850\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['title', 'content'],\n",
       "        num_rows: 1679\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 7
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-23T13:51:39.074269Z",
     "start_time": "2024-09-23T13:51:39.065838Z"
    }
   },
   "source": [
    "datasets[\"train\"][0]"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'title': '望海楼美国打“台湾牌”是危险的赌博',\n",
       " 'content': '近期，美国国会众院通过法案，重申美国对台湾的承诺。对此，中国外交部发言人表示，有关法案严重违反一个中国原则和中美三个联合公报规定，粗暴干涉中国内政，中方对此坚决反对并已向美方提出严正交涉。\\n事实上，中国高度关注美国国内打“台湾牌”、挑战一中原则的危险动向。近年来，作为“亲台”势力大本营的美国国会动作不断，先后通过“与台湾交往法”“亚洲再保证倡议法”等一系列“挺台”法案，“2019财年国防授权法案”也多处触及台湾问题。今年3月，美参院亲台议员再抛“台湾保证法”草案。众院议员继而在4月提出众院版的草案并在近期通过。上述法案的核心目标是强化美台关系，并将台作为美“印太战略”的重要伙伴。同时，“亲台”议员还有意制造事端。今年2月，5名共和党参议员致信众议院议长，促其邀请台湾地区领导人在国会上发表讲话。这一动议显然有悖于美国与台湾的非官方关系，其用心是实质性改变美台关系定位。\\n上述动向出现并非偶然。在中美建交40周年之际，两国关系摩擦加剧，所谓“中国威胁论”再次沉渣泛起。美国对华认知出现严重偏差，对华政策中负面因素上升，保守人士甚至成立了“当前中国威胁委员会”。在此背景下，美国将台海关系作为战略抓手，通过打“台湾牌”在双边关系中增加筹码。特朗普就任后，国会对总统外交政策的约束力和塑造力加强。其实国会推动通过涉台法案对行政部门不具约束力，美政府在2018年并未提升美台官员互访级别，美军舰也没有“访问”台湾港口，保持着某种克制。但从美总统签署国会通过的法案可以看出，国会对外交产生了影响。立法也为政府对台政策提供更大空间。\\n然而，美国需要认真衡量打“台湾牌”成本。首先是美国应对危机的代价。美方官员和学者已明确发出警告，美国卷入台湾问题得不偿失。美国学者曾在媒体发文指出，如果台海爆发危机，美国可能需要“援助”台湾，进而导致新的冷战乃至与中国大陆的冲突。但如果美国让台湾自己面对，则有损美国的信誉，影响美盟友对同盟关系的支持。其次是对中美关系的危害。历史证明，中美合则两利、斗则两伤。中美关系是当今世界最重要的双边关系之一，保持中美关系的稳定发展，不仅符合两国和两国人民的根本利益，也是国际社会的普遍期待。美国蓄意挑战台湾问题的底线，加剧中美关系的复杂性和不确定性，损害两国在重要领域合作，损人又害己。\\n美国打“台湾牌”是一场危险的赌博。台湾问题是中国核心利益，中国政府和人民决不会对此坐视不理。中国敦促美方恪守一个中国原则和中美三个联合公报规定，阻止美国会审议推进有关法案，妥善处理涉台问题。美国悬崖勒马，才是明智之举。\\n（作者系中国国际问题研究院国际战略研究所副所长）'}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 8
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets[\"train\"][:2]"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-23T13:51:52.215663Z",
     "start_time": "2024-09-23T13:51:52.201281Z"
    }
   },
   "source": [
    "datasets[\"train\"][\"title\"][:5]"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['望海楼美国打“台湾牌”是危险的赌博',\n",
       " '大力推进高校治理能力建设',\n",
       " '坚持事业为上选贤任能',\n",
       " '“大朋友”的话儿记心头',\n",
       " '用好可持续发展这把“金钥匙”']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 11
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-23T13:51:44.421731Z",
     "start_time": "2024-09-23T13:51:44.416284Z"
    }
   },
   "source": [
    "datasets[\"train\"].column_names"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['title', 'content']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 9
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-23T13:51:46.929739Z",
     "start_time": "2024-09-23T13:51:46.925276Z"
    }
   },
   "source": [
    "datasets[\"train\"].features"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'title': Value(dtype='string', id=None),\n",
       " 'content': Value(dtype='string', id=None)}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 10
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 数据集划分"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-23T13:51:58.898092Z",
     "start_time": "2024-09-23T13:51:58.878748Z"
    }
   },
   "source": [
    "dataset = datasets[\"train\"]\n",
    "dataset.train_test_split(test_size=0.1)"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['title', 'content'],\n",
       "        num_rows: 5265\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['title', 'content'],\n",
       "        num_rows: 585\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 12
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-23T13:52:20.010173Z",
     "start_time": "2024-09-23T13:52:19.991822Z"
    }
   },
   "source": [
    "dataset = boolq_dataset[\"train\"]\n",
    "dataset.train_test_split(test_size=0.1, stratify_by_column=\"label\")     # 分类数据集可以按照比例划分"
   ],
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'boolq_dataset' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[13], line 1\u001B[0m\n\u001B[1;32m----> 1\u001B[0m dataset \u001B[38;5;241m=\u001B[39m boolq_dataset[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtrain\u001B[39m\u001B[38;5;124m\"\u001B[39m]\n\u001B[0;32m      2\u001B[0m dataset\u001B[38;5;241m.\u001B[39mtrain_test_split(test_size\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m0.1\u001B[39m, stratify_by_column\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mlabel\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
      "\u001B[1;31mNameError\u001B[0m: name 'boolq_dataset' is not defined"
     ]
    }
   ],
   "execution_count": 13
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 数据选取与过滤"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-23T13:52:29.260854Z",
     "start_time": "2024-09-23T13:52:29.253414Z"
    }
   },
   "source": [
    "# 选取\n",
    "datasets[\"train\"].select([0, 1])"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['title', 'content'],\n",
       "    num_rows: 2\n",
       "})"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 14
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-23T13:52:37.410874Z",
     "start_time": "2024-09-23T13:52:37.318121Z"
    }
   },
   "source": [
    "# 过滤\n",
    "filter_dataset = datasets[\"train\"].filter(lambda example: \"中国\" in example[\"title\"])"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Filter:   0%|          | 0/5850 [00:00<?, ? examples/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "1027bcffbd064f9d850543a519069004"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 15
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-23T13:52:43.033537Z",
     "start_time": "2024-09-23T13:52:43.024085Z"
    }
   },
   "source": [
    "filter_dataset[\"title\"][:5]"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['聚焦两会，世界探寻中国成功秘诀',\n",
       " '望海楼中国经济的信心来自哪里',\n",
       " '“中国奇迹”助力世界减贫跑出加速度',\n",
       " '和音瞩目历史交汇点上的中国',\n",
       " '中国风采感染世界']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 16
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 数据映射"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-23T13:52:56.853177Z",
     "start_time": "2024-09-23T13:52:56.849210Z"
    }
   },
   "source": [
    "def add_prefix(example):\n",
    "    example[\"title\"] = 'Prefix: ' + example[\"title\"]\n",
    "    return example"
   ],
   "outputs": [],
   "execution_count": 17
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-23T13:53:05.612932Z",
     "start_time": "2024-09-23T13:53:05.170995Z"
    }
   },
   "source": [
    "prefix_dataset = datasets.map(add_prefix)\n",
    "prefix_dataset[\"train\"][:10][\"title\"]"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Map:   0%|          | 0/5850 [00:00<?, ? examples/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "1fdbabc5762342918c488a08cad2965a"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Map:   0%|          | 0/1679 [00:00<?, ? examples/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "ab770842251e43caa318532770e3d7b4"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "['Prefix: 望海楼美国打“台湾牌”是危险的赌博',\n",
       " 'Prefix: 大力推进高校治理能力建设',\n",
       " 'Prefix: 坚持事业为上选贤任能',\n",
       " 'Prefix: “大朋友”的话儿记心头',\n",
       " 'Prefix: 用好可持续发展这把“金钥匙”',\n",
       " 'Prefix: 跨越雄关，我们走在大路上',\n",
       " 'Prefix: 脱贫奇迹彰显政治优势',\n",
       " 'Prefix: 拱卫亿万人共同的绿色梦想',\n",
       " 'Prefix: 为党育人、为国育才',\n",
       " 'Prefix: 净化网络语言']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 18
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-23T13:53:26.134031Z",
     "start_time": "2024-09-23T13:53:17.476562Z"
    }
   },
   "source": [
    "from transformers import AutoTokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-chinese\")\n",
    "def preprocess_function(example, tokenizer=tokenizer):\n",
    "    model_inputs = tokenizer(example[\"content\"], max_length=512, truncation=True)\n",
    "    labels = tokenizer(example[\"title\"], max_length=32, truncation=True)\n",
    "    # label就是title编码的结果\n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    return model_inputs"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/49.0 [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "bc4db07934484196a7019e1679da8cc0"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\wdx\\.conda\\envs\\wdx_torch\\Lib\\site-packages\\huggingface_hub\\file_download.py:147: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\wdx\\.cache\\huggingface\\hub\\models--bert-base-chinese. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "config.json:   0%|          | 0.00/624 [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "5325064cd8ef49ffacd6ee7253786f11"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/110k [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "fba0d42e824a47c3b633440d4bfa2d90"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/269k [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "fa4570652400471b8f48337c5528222c"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\wdx\\.conda\\envs\\wdx_torch\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "execution_count": 19
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-23T13:54:00.880741Z",
     "start_time": "2024-09-23T13:53:40.837408Z"
    }
   },
   "source": [
    "processed_datasets = datasets.map(preprocess_function)\n",
    "processed_datasets"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Map:   0%|          | 0/5850 [00:00<?, ? examples/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "713caf28e89b47b7af383c44a92ce204"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Map:   0%|          | 0/1679 [00:00<?, ? examples/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "8e9e1303d8f34df787c22a056c85274c"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['title', 'content', 'input_ids', 'token_type_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 5850\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['title', 'content', 'input_ids', 'token_type_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 1679\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 20
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-23T14:00:31.420968Z",
     "start_time": "2024-09-23T14:00:10.514333Z"
    }
   },
   "source": [
    "processed_datasets = datasets.map(preprocess_function, num_proc=4)\n",
    "processed_datasets"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Map (num_proc=4):   0%|          | 0/5850 [00:00<?, ? examples/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "b6be3507fd2e4e1587062bedabadae4d"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Map (num_proc=4):   0%|          | 0/1679 [00:00<?, ? examples/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "c5f5589c2b8f4f6fad8454c41d8607f9"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['title', 'content', 'input_ids', 'token_type_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 5850\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['title', 'content', 'input_ids', 'token_type_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 1679\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 21
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-23T14:00:59.031148Z",
     "start_time": "2024-09-23T14:00:40.259909Z"
    }
   },
   "source": [
    "processed_datasets = datasets.map(preprocess_function, batched=True)\n",
    "processed_datasets"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Map:   0%|          | 0/5850 [00:00<?, ? examples/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "02bba2a2c7f547a7bb17351865da568f"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Map:   0%|          | 0/1679 [00:00<?, ? examples/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "7433194a96554318b688231352371c82"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['title', 'content', 'input_ids', 'token_type_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 5850\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['title', 'content', 'input_ids', 'token_type_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 1679\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 22
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-23T14:01:26.283814Z",
     "start_time": "2024-09-23T14:01:07.588543Z"
    }
   },
   "source": [
    "processed_datasets = datasets.map(preprocess_function, batched=True, remove_columns=datasets[\"train\"].column_names)\n",
    "processed_datasets"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Map:   0%|          | 0/5850 [00:00<?, ? examples/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "aa74d80685ee4d089d47d312db3b6163"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Map:   0%|          | 0/1679 [00:00<?, ? examples/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "71b927f647f2450f8c9e7c844480a795"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['input_ids', 'token_type_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 5850\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['input_ids', 'token_type_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 1679\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 23
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 保存与加载"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-23T14:01:39.640458Z",
     "start_time": "2024-09-23T14:01:39.579450Z"
    }
   },
   "source": [
    "processed_datasets.save_to_disk(\"./processed_data\")"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/5850 [00:00<?, ? examples/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "f7e38263de4b4a32af785555a4571d46"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/1679 [00:00<?, ? examples/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "0afb4f04123e4690b295802ec773891e"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 24
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-23T14:05:08.483043Z",
     "start_time": "2024-09-23T14:05:08.478627Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import os\n",
    "print(os.getcwd())"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F:\\study\\huggfaceingTest\\Test1\\05-datasets\n"
     ]
    }
   ],
   "execution_count": 25
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-23T14:06:08.866545Z",
     "start_time": "2024-09-23T14:06:08.851676Z"
    }
   },
   "source": [
    "processed_datasets = load_from_disk(\"./processed_data\")\n",
    "processed_datasets"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['input_ids', 'token_type_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 5850\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['input_ids', 'token_type_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 1679\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 26
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 加载本地数据集"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 直接加载文件作为数据集"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-23T14:18:16.568292Z",
     "start_time": "2024-09-23T14:18:16.437348Z"
    }
   },
   "source": [
    "dataset = load_dataset(\"csv\", data_files=\"./ChnSentiCorp_htl_all.csv\", split=\"train\")\n",
    "dataset"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "c8ab8a3667e44e3da9647c983fd68b12"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['label', 'review'],\n",
       "    num_rows: 7766\n",
       "})"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 27
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-23T14:18:32.351289Z",
     "start_time": "2024-09-23T14:18:30.704429Z"
    }
   },
   "source": [
    "dataset = Dataset.from_csv(\"./ChnSentiCorp_htl_all.csv\")\n",
    "dataset"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "df2c12d3e2ad4ba7ad2c58612904b694"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "PermissionError",
     "evalue": "[WinError 5] 拒绝访问。: 'C:/Users/wdx/.cache/huggingface/datasets/csv/default-34b2f95a4687543a/0.0.0\\\\9ea1179385ff7ad1e756d327ffccaa3b801175702a2d91528226ba2c66873f52\\\\csv-train.arrow'",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mPermissionError\u001B[0m                           Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[28], line 1\u001B[0m\n\u001B[1;32m----> 1\u001B[0m dataset \u001B[38;5;241m=\u001B[39m Dataset\u001B[38;5;241m.\u001B[39mfrom_csv(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m./ChnSentiCorp_htl_all.csv\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m      2\u001B[0m dataset\n",
      "File \u001B[1;32m~\\.conda\\envs\\wdx_torch\\Lib\\site-packages\\datasets\\arrow_dataset.py:1024\u001B[0m, in \u001B[0;36mDataset.from_csv\u001B[1;34m(path_or_paths, split, features, cache_dir, keep_in_memory, num_proc, **kwargs)\u001B[0m\n\u001B[0;32m   1013\u001B[0m \u001B[38;5;66;03m# Dynamic import to avoid circular dependency\u001B[39;00m\n\u001B[0;32m   1014\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mio\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mcsv\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m CsvDatasetReader\n\u001B[0;32m   1016\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m CsvDatasetReader(\n\u001B[0;32m   1017\u001B[0m     path_or_paths,\n\u001B[0;32m   1018\u001B[0m     split\u001B[38;5;241m=\u001B[39msplit,\n\u001B[0;32m   1019\u001B[0m     features\u001B[38;5;241m=\u001B[39mfeatures,\n\u001B[0;32m   1020\u001B[0m     cache_dir\u001B[38;5;241m=\u001B[39mcache_dir,\n\u001B[0;32m   1021\u001B[0m     keep_in_memory\u001B[38;5;241m=\u001B[39mkeep_in_memory,\n\u001B[0;32m   1022\u001B[0m     num_proc\u001B[38;5;241m=\u001B[39mnum_proc,\n\u001B[0;32m   1023\u001B[0m     \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs,\n\u001B[1;32m-> 1024\u001B[0m )\u001B[38;5;241m.\u001B[39mread()\n",
      "File \u001B[1;32m~\\.conda\\envs\\wdx_torch\\Lib\\site-packages\\datasets\\io\\csv.py:56\u001B[0m, in \u001B[0;36mCsvDatasetReader.read\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m     53\u001B[0m     verification_mode \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[0;32m     54\u001B[0m     base_path \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m---> 56\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mbuilder\u001B[38;5;241m.\u001B[39mdownload_and_prepare(\n\u001B[0;32m     57\u001B[0m         download_config\u001B[38;5;241m=\u001B[39mdownload_config,\n\u001B[0;32m     58\u001B[0m         download_mode\u001B[38;5;241m=\u001B[39mdownload_mode,\n\u001B[0;32m     59\u001B[0m         verification_mode\u001B[38;5;241m=\u001B[39mverification_mode,\n\u001B[0;32m     60\u001B[0m         base_path\u001B[38;5;241m=\u001B[39mbase_path,\n\u001B[0;32m     61\u001B[0m         num_proc\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mnum_proc,\n\u001B[0;32m     62\u001B[0m     )\n\u001B[0;32m     63\u001B[0m     dataset \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mbuilder\u001B[38;5;241m.\u001B[39mas_dataset(\n\u001B[0;32m     64\u001B[0m         split\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39msplit, verification_mode\u001B[38;5;241m=\u001B[39mverification_mode, in_memory\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mkeep_in_memory\n\u001B[0;32m     65\u001B[0m     )\n\u001B[0;32m     66\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m dataset\n",
      "File \u001B[1;32m~\\.conda\\envs\\wdx_torch\\Lib\\site-packages\\datasets\\builder.py:915\u001B[0m, in \u001B[0;36mDatasetBuilder.download_and_prepare\u001B[1;34m(self, output_dir, download_config, download_mode, verification_mode, dl_manager, base_path, file_format, max_shard_size, num_proc, storage_options, **download_and_prepare_kwargs)\u001B[0m\n\u001B[0;32m    912\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_check_manual_download(dl_manager)\n\u001B[0;32m    914\u001B[0m \u001B[38;5;66;03m# Create a tmp dir and rename to self._output_dir on successful exit.\u001B[39;00m\n\u001B[1;32m--> 915\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m incomplete_dir(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_output_dir) \u001B[38;5;28;01mas\u001B[39;00m tmp_output_dir:\n\u001B[0;32m    916\u001B[0m     \u001B[38;5;66;03m# Temporarily assign _output_dir to tmp_data_dir to avoid having to forward\u001B[39;00m\n\u001B[0;32m    917\u001B[0m     \u001B[38;5;66;03m# it to every sub function.\u001B[39;00m\n\u001B[0;32m    918\u001B[0m     \u001B[38;5;28;01mwith\u001B[39;00m temporary_assignment(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m_output_dir\u001B[39m\u001B[38;5;124m\"\u001B[39m, tmp_output_dir):\n\u001B[0;32m    919\u001B[0m         prepare_split_kwargs \u001B[38;5;241m=\u001B[39m {\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mfile_format\u001B[39m\u001B[38;5;124m\"\u001B[39m: file_format}\n",
      "File \u001B[1;32m~\\.conda\\envs\\wdx_torch\\Lib\\contextlib.py:144\u001B[0m, in \u001B[0;36m_GeneratorContextManager.__exit__\u001B[1;34m(self, typ, value, traceback)\u001B[0m\n\u001B[0;32m    142\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m typ \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m    143\u001B[0m     \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m--> 144\u001B[0m         \u001B[38;5;28mnext\u001B[39m(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mgen)\n\u001B[0;32m    145\u001B[0m     \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mStopIteration\u001B[39;00m:\n\u001B[0;32m    146\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;01mFalse\u001B[39;00m\n",
      "File \u001B[1;32m~\\.conda\\envs\\wdx_torch\\Lib\\site-packages\\datasets\\builder.py:891\u001B[0m, in \u001B[0;36mDatasetBuilder.download_and_prepare.<locals>.incomplete_dir\u001B[1;34m(dirname)\u001B[0m\n\u001B[0;32m    889\u001B[0m \u001B[38;5;28;01myield\u001B[39;00m tmp_dir\n\u001B[0;32m    890\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m os\u001B[38;5;241m.\u001B[39mpath\u001B[38;5;241m.\u001B[39misdir(dirname):\n\u001B[1;32m--> 891\u001B[0m     shutil\u001B[38;5;241m.\u001B[39mrmtree(dirname)\n\u001B[0;32m    892\u001B[0m \u001B[38;5;66;03m# LocalFileSystem.mv does copy + rm, it is more efficient to simply rename a local directory\u001B[39;00m\n\u001B[0;32m    893\u001B[0m shutil\u001B[38;5;241m.\u001B[39mmove(tmp_dir, dirname)\n",
      "File \u001B[1;32m~\\.conda\\envs\\wdx_torch\\Lib\\shutil.py:787\u001B[0m, in \u001B[0;36mrmtree\u001B[1;34m(path, ignore_errors, onerror, dir_fd)\u001B[0m\n\u001B[0;32m    785\u001B[0m     \u001B[38;5;66;03m# can't continue even if onerror hook returns\u001B[39;00m\n\u001B[0;32m    786\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m\n\u001B[1;32m--> 787\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m _rmtree_unsafe(path, onerror)\n",
      "File \u001B[1;32m~\\.conda\\envs\\wdx_torch\\Lib\\shutil.py:629\u001B[0m, in \u001B[0;36m_rmtree_unsafe\u001B[1;34m(path, onerror)\u001B[0m\n\u001B[0;32m    627\u001B[0m         onerror(os\u001B[38;5;241m.\u001B[39mpath\u001B[38;5;241m.\u001B[39mislink, fullname, sys\u001B[38;5;241m.\u001B[39mexc_info())\n\u001B[0;32m    628\u001B[0m         \u001B[38;5;28;01mcontinue\u001B[39;00m\n\u001B[1;32m--> 629\u001B[0m     _rmtree_unsafe(fullname, onerror)\n\u001B[0;32m    630\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m    631\u001B[0m     \u001B[38;5;28;01mtry\u001B[39;00m:\n",
      "File \u001B[1;32m~\\.conda\\envs\\wdx_torch\\Lib\\shutil.py:634\u001B[0m, in \u001B[0;36m_rmtree_unsafe\u001B[1;34m(path, onerror)\u001B[0m\n\u001B[0;32m    632\u001B[0m             os\u001B[38;5;241m.\u001B[39munlink(fullname)\n\u001B[0;32m    633\u001B[0m         \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mOSError\u001B[39;00m:\n\u001B[1;32m--> 634\u001B[0m             onerror(os\u001B[38;5;241m.\u001B[39munlink, fullname, sys\u001B[38;5;241m.\u001B[39mexc_info())\n\u001B[0;32m    635\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m    636\u001B[0m     os\u001B[38;5;241m.\u001B[39mrmdir(path)\n",
      "File \u001B[1;32m~\\.conda\\envs\\wdx_torch\\Lib\\shutil.py:632\u001B[0m, in \u001B[0;36m_rmtree_unsafe\u001B[1;34m(path, onerror)\u001B[0m\n\u001B[0;32m    630\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m    631\u001B[0m     \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m--> 632\u001B[0m         os\u001B[38;5;241m.\u001B[39munlink(fullname)\n\u001B[0;32m    633\u001B[0m     \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mOSError\u001B[39;00m:\n\u001B[0;32m    634\u001B[0m         onerror(os\u001B[38;5;241m.\u001B[39munlink, fullname, sys\u001B[38;5;241m.\u001B[39mexc_info())\n",
      "\u001B[1;31mPermissionError\u001B[0m: [WinError 5] 拒绝访问。: 'C:/Users/wdx/.cache/huggingface/datasets/csv/default-34b2f95a4687543a/0.0.0\\\\9ea1179385ff7ad1e756d327ffccaa3b801175702a2d91528226ba2c66873f52\\\\csv-train.arrow'"
     ]
    }
   ],
   "execution_count": 28
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 加载文件夹内全部文件作为数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset(\"csv\", data_files=[\"./all_data/ChnSentiCorp_htl_all.csv\", \"./all_data/ChnSentiCorp_htl_all copy.csv\"], split='train')\n",
    "dataset"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 通过预先加载的其他格式转换加载数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "data = pd.read_csv(\"./ChnSentiCorp_htl_all.csv\")\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = Dataset.from_pandas(data)\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List格式的数据需要内嵌{}，明确数据字段\n",
    "data = [{\"text\": \"abc\"}, {\"text\": \"def\"}]\n",
    "# data = [\"abc\", \"def\"]\n",
    "Dataset.from_list(data)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 通过自定义加载脚本加载数据集"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-23T14:32:05.038166Z",
     "start_time": "2024-09-23T14:32:04.939958Z"
    }
   },
   "source": [
    "load_dataset(\"json\", data_files=\"./cmrc2018_trial.json\", field=\"data\")"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "08c397109c8d4df68a382375162e38f8"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['paragraphs', 'id', 'title'],\n",
       "        num_rows: 256\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 30
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-23T14:32:09.396244Z",
     "start_time": "2024-09-23T14:32:09.286131Z"
    }
   },
   "source": [
    "dataset = load_dataset(\"./load_script.py\", split=\"train\")\n",
    "dataset"
   ],
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "The repository for load_script contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at https://hf.co/datasets/load_script.\nPlease pass the argument `trust_remote_code=True` to allow custom code to be run.",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mAttributeError\u001B[0m                            Traceback (most recent call last)",
      "File \u001B[1;32m~\\.conda\\envs\\wdx_torch\\Lib\\site-packages\\datasets\\load.py:115\u001B[0m, in \u001B[0;36mresolve_trust_remote_code\u001B[1;34m(trust_remote_code, repo_id)\u001B[0m\n\u001B[0;32m    114\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m--> 115\u001B[0m     signal\u001B[38;5;241m.\u001B[39msignal(signal\u001B[38;5;241m.\u001B[39mSIGALRM, _raise_timeout_error)\n\u001B[0;32m    116\u001B[0m     signal\u001B[38;5;241m.\u001B[39malarm(config\u001B[38;5;241m.\u001B[39mTIME_OUT_REMOTE_CODE)\n",
      "\u001B[1;31mAttributeError\u001B[0m: module 'signal' has no attribute 'SIGALRM'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001B[1;31mValueError\u001B[0m                                Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[31], line 1\u001B[0m\n\u001B[1;32m----> 1\u001B[0m dataset \u001B[38;5;241m=\u001B[39m load_dataset(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m./load_script.py\u001B[39m\u001B[38;5;124m\"\u001B[39m, split\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtrain\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m      2\u001B[0m dataset\n",
      "File \u001B[1;32m~\\.conda\\envs\\wdx_torch\\Lib\\site-packages\\datasets\\load.py:2074\u001B[0m, in \u001B[0;36mload_dataset\u001B[1;34m(path, name, data_dir, data_files, split, cache_dir, features, download_config, download_mode, verification_mode, keep_in_memory, save_infos, revision, token, streaming, num_proc, storage_options, trust_remote_code, **config_kwargs)\u001B[0m\n\u001B[0;32m   2069\u001B[0m verification_mode \u001B[38;5;241m=\u001B[39m VerificationMode(\n\u001B[0;32m   2070\u001B[0m     (verification_mode \u001B[38;5;129;01mor\u001B[39;00m VerificationMode\u001B[38;5;241m.\u001B[39mBASIC_CHECKS) \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m save_infos \u001B[38;5;28;01melse\u001B[39;00m VerificationMode\u001B[38;5;241m.\u001B[39mALL_CHECKS\n\u001B[0;32m   2071\u001B[0m )\n\u001B[0;32m   2073\u001B[0m \u001B[38;5;66;03m# Create a dataset builder\u001B[39;00m\n\u001B[1;32m-> 2074\u001B[0m builder_instance \u001B[38;5;241m=\u001B[39m load_dataset_builder(\n\u001B[0;32m   2075\u001B[0m     path\u001B[38;5;241m=\u001B[39mpath,\n\u001B[0;32m   2076\u001B[0m     name\u001B[38;5;241m=\u001B[39mname,\n\u001B[0;32m   2077\u001B[0m     data_dir\u001B[38;5;241m=\u001B[39mdata_dir,\n\u001B[0;32m   2078\u001B[0m     data_files\u001B[38;5;241m=\u001B[39mdata_files,\n\u001B[0;32m   2079\u001B[0m     cache_dir\u001B[38;5;241m=\u001B[39mcache_dir,\n\u001B[0;32m   2080\u001B[0m     features\u001B[38;5;241m=\u001B[39mfeatures,\n\u001B[0;32m   2081\u001B[0m     download_config\u001B[38;5;241m=\u001B[39mdownload_config,\n\u001B[0;32m   2082\u001B[0m     download_mode\u001B[38;5;241m=\u001B[39mdownload_mode,\n\u001B[0;32m   2083\u001B[0m     revision\u001B[38;5;241m=\u001B[39mrevision,\n\u001B[0;32m   2084\u001B[0m     token\u001B[38;5;241m=\u001B[39mtoken,\n\u001B[0;32m   2085\u001B[0m     storage_options\u001B[38;5;241m=\u001B[39mstorage_options,\n\u001B[0;32m   2086\u001B[0m     trust_remote_code\u001B[38;5;241m=\u001B[39mtrust_remote_code,\n\u001B[0;32m   2087\u001B[0m     _require_default_config_name\u001B[38;5;241m=\u001B[39mname \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m,\n\u001B[0;32m   2088\u001B[0m     \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mconfig_kwargs,\n\u001B[0;32m   2089\u001B[0m )\n\u001B[0;32m   2091\u001B[0m \u001B[38;5;66;03m# Return iterable dataset in case of streaming\u001B[39;00m\n\u001B[0;32m   2092\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m streaming:\n",
      "File \u001B[1;32m~\\.conda\\envs\\wdx_torch\\Lib\\site-packages\\datasets\\load.py:1795\u001B[0m, in \u001B[0;36mload_dataset_builder\u001B[1;34m(path, name, data_dir, data_files, cache_dir, features, download_config, download_mode, revision, token, storage_options, trust_remote_code, _require_default_config_name, **config_kwargs)\u001B[0m\n\u001B[0;32m   1793\u001B[0m     download_config \u001B[38;5;241m=\u001B[39m download_config\u001B[38;5;241m.\u001B[39mcopy() \u001B[38;5;28;01mif\u001B[39;00m download_config \u001B[38;5;28;01melse\u001B[39;00m DownloadConfig()\n\u001B[0;32m   1794\u001B[0m     download_config\u001B[38;5;241m.\u001B[39mstorage_options\u001B[38;5;241m.\u001B[39mupdate(storage_options)\n\u001B[1;32m-> 1795\u001B[0m dataset_module \u001B[38;5;241m=\u001B[39m dataset_module_factory(\n\u001B[0;32m   1796\u001B[0m     path,\n\u001B[0;32m   1797\u001B[0m     revision\u001B[38;5;241m=\u001B[39mrevision,\n\u001B[0;32m   1798\u001B[0m     download_config\u001B[38;5;241m=\u001B[39mdownload_config,\n\u001B[0;32m   1799\u001B[0m     download_mode\u001B[38;5;241m=\u001B[39mdownload_mode,\n\u001B[0;32m   1800\u001B[0m     data_dir\u001B[38;5;241m=\u001B[39mdata_dir,\n\u001B[0;32m   1801\u001B[0m     data_files\u001B[38;5;241m=\u001B[39mdata_files,\n\u001B[0;32m   1802\u001B[0m     cache_dir\u001B[38;5;241m=\u001B[39mcache_dir,\n\u001B[0;32m   1803\u001B[0m     trust_remote_code\u001B[38;5;241m=\u001B[39mtrust_remote_code,\n\u001B[0;32m   1804\u001B[0m     _require_default_config_name\u001B[38;5;241m=\u001B[39m_require_default_config_name,\n\u001B[0;32m   1805\u001B[0m     _require_custom_configs\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mbool\u001B[39m(config_kwargs),\n\u001B[0;32m   1806\u001B[0m )\n\u001B[0;32m   1807\u001B[0m \u001B[38;5;66;03m# Get dataset builder class from the processing script\u001B[39;00m\n\u001B[0;32m   1808\u001B[0m builder_kwargs \u001B[38;5;241m=\u001B[39m dataset_module\u001B[38;5;241m.\u001B[39mbuilder_kwargs\n",
      "File \u001B[1;32m~\\.conda\\envs\\wdx_torch\\Lib\\site-packages\\datasets\\load.py:1560\u001B[0m, in \u001B[0;36mdataset_module_factory\u001B[1;34m(path, revision, download_config, download_mode, dynamic_modules_path, data_dir, data_files, cache_dir, trust_remote_code, _require_default_config_name, _require_custom_configs, **download_kwargs)\u001B[0m\n\u001B[0;32m   1553\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m path\u001B[38;5;241m.\u001B[39mendswith(filename):\n\u001B[0;32m   1554\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m os\u001B[38;5;241m.\u001B[39mpath\u001B[38;5;241m.\u001B[39misfile(path):\n\u001B[0;32m   1555\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m LocalDatasetModuleFactoryWithScript(\n\u001B[0;32m   1556\u001B[0m             path,\n\u001B[0;32m   1557\u001B[0m             download_mode\u001B[38;5;241m=\u001B[39mdownload_mode,\n\u001B[0;32m   1558\u001B[0m             dynamic_modules_path\u001B[38;5;241m=\u001B[39mdynamic_modules_path,\n\u001B[0;32m   1559\u001B[0m             trust_remote_code\u001B[38;5;241m=\u001B[39mtrust_remote_code,\n\u001B[1;32m-> 1560\u001B[0m         )\u001B[38;5;241m.\u001B[39mget_module()\n\u001B[0;32m   1561\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m   1562\u001B[0m         \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mFileNotFoundError\u001B[39;00m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mCouldn\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mt find a dataset script at \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mrelative_to_absolute_path(path)\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m)\n",
      "File \u001B[1;32m~\\.conda\\envs\\wdx_torch\\Lib\\site-packages\\datasets\\load.py:754\u001B[0m, in \u001B[0;36mLocalDatasetModuleFactoryWithScript.get_module\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m    747\u001B[0m importable_file_path \u001B[38;5;241m=\u001B[39m _get_importable_file_path(\n\u001B[0;32m    748\u001B[0m     dynamic_modules_path\u001B[38;5;241m=\u001B[39mdynamic_modules_path,\n\u001B[0;32m    749\u001B[0m     module_namespace\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mdatasets\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[0;32m    750\u001B[0m     subdirectory_name\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mhash\u001B[39m,\n\u001B[0;32m    751\u001B[0m     name\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mname,\n\u001B[0;32m    752\u001B[0m )\n\u001B[0;32m    753\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m os\u001B[38;5;241m.\u001B[39mpath\u001B[38;5;241m.\u001B[39mexists(importable_file_path):\n\u001B[1;32m--> 754\u001B[0m     trust_remote_code \u001B[38;5;241m=\u001B[39m resolve_trust_remote_code(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtrust_remote_code, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mname)\n\u001B[0;32m    755\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m trust_remote_code:\n\u001B[0;32m    756\u001B[0m         _create_importable_file(\n\u001B[0;32m    757\u001B[0m             local_path\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mpath,\n\u001B[0;32m    758\u001B[0m             local_imports\u001B[38;5;241m=\u001B[39mlocal_imports,\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    764\u001B[0m             download_mode\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdownload_mode,\n\u001B[0;32m    765\u001B[0m         )\n",
      "File \u001B[1;32m~\\.conda\\envs\\wdx_torch\\Lib\\site-packages\\datasets\\load.py:131\u001B[0m, in \u001B[0;36mresolve_trust_remote_code\u001B[1;34m(trust_remote_code, repo_id)\u001B[0m\n\u001B[0;32m    128\u001B[0m         signal\u001B[38;5;241m.\u001B[39malarm(\u001B[38;5;241m0\u001B[39m)\n\u001B[0;32m    129\u001B[0m     \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mException\u001B[39;00m:\n\u001B[0;32m    130\u001B[0m         \u001B[38;5;66;03m# OS which does not support signal.SIGALRM\u001B[39;00m\n\u001B[1;32m--> 131\u001B[0m         \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\n\u001B[0;32m    132\u001B[0m             \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mThe repository for \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mrepo_id\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m contains custom code which must be executed to correctly \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    133\u001B[0m             \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mload the dataset. You can inspect the repository content at https://hf.co/datasets/\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mrepo_id\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m.\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    134\u001B[0m             \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mPlease pass the argument `trust_remote_code=True` to allow custom code to be run.\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    135\u001B[0m         )\n\u001B[0;32m    136\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m    137\u001B[0m     \u001B[38;5;66;03m# For the CI which might put the timeout at 0\u001B[39;00m\n\u001B[0;32m    138\u001B[0m     _raise_timeout_error(\u001B[38;5;28;01mNone\u001B[39;00m, \u001B[38;5;28;01mNone\u001B[39;00m)\n",
      "\u001B[1;31mValueError\u001B[0m: The repository for load_script contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at https://hf.co/datasets/load_script.\nPlease pass the argument `trust_remote_code=True` to allow custom code to be run."
     ]
    }
   ],
   "execution_count": 31
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset[0]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset with DataCollator"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-23T14:42:12.515757Z",
     "start_time": "2024-09-23T14:42:12.493439Z"
    }
   },
   "source": [
    "from transformers import  DataCollatorWithPadding"
   ],
   "outputs": [],
   "execution_count": 32
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-23T14:42:15.319424Z",
     "start_time": "2024-09-23T14:42:15.244527Z"
    }
   },
   "source": [
    "dataset = load_dataset(\"csv\", data_files=\"./ChnSentiCorp_htl_all.csv\", split='train')\n",
    "dataset = dataset.filter(lambda x: x[\"review\"] is not None)\n",
    "dataset"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Filter:   0%|          | 0/7766 [00:00<?, ? examples/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "292ae7b892e64332b6272f18f055d4ab"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['label', 'review'],\n",
       "    num_rows: 7765\n",
       "})"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 33
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-23T14:42:19.353080Z",
     "start_time": "2024-09-23T14:42:19.349608Z"
    }
   },
   "source": [
    "def process_function(examples):\n",
    "    tokenized_examples = tokenizer(examples[\"review\"], max_length=128, truncation=True)\n",
    "    tokenized_examples[\"labels\"] = examples[\"label\"]\n",
    "    return tokenized_examples"
   ],
   "outputs": [],
   "execution_count": 34
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-23T14:42:24.765117Z",
     "start_time": "2024-09-23T14:42:22.464666Z"
    }
   },
   "source": [
    "tokenized_dataset = dataset.map(process_function, batched=True, remove_columns=dataset.column_names)\n",
    "tokenized_dataset"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Map:   0%|          | 0/7765 [00:00<?, ? examples/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "6556a2ce9c6f4d3e9f3fff3778ec2df2"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['input_ids', 'token_type_ids', 'attention_mask', 'labels'],\n",
       "    num_rows: 7765\n",
       "})"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 35
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-23T14:42:27.620578Z",
     "start_time": "2024-09-23T14:42:27.615121Z"
    }
   },
   "source": [
    "print(tokenized_dataset[:3])"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': [[101, 6655, 4895, 2335, 3763, 1062, 6662, 6772, 6818, 117, 852, 3221, 1062, 769, 2900, 4850, 679, 2190, 117, 1963, 3362, 3221, 107, 5918, 7355, 5296, 107, 4638, 6413, 117, 833, 7478, 2382, 7937, 4172, 119, 2456, 6379, 4500, 1166, 4638, 6662, 5296, 119, 2791, 7313, 6772, 711, 5042, 1296, 119, 102], [101, 1555, 1218, 1920, 2414, 2791, 8024, 2791, 7313, 2523, 1920, 8024, 2414, 3300, 100, 2160, 8024, 3146, 860, 2697, 6230, 5307, 3845, 2141, 2669, 679, 7231, 106, 102], [101, 3193, 7623, 1922, 2345, 8024, 3187, 6389, 1343, 1914, 2208, 782, 8024, 6929, 6804, 738, 679, 1217, 7608, 1501, 4638, 511, 6983, 2421, 2418, 6421, 7028, 6228, 671, 678, 6821, 702, 7309, 7579, 749, 511, 2791, 7313, 3315, 6716, 2523, 1962, 511, 102]], 'token_type_ids': [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]], 'labels': [1, 1, 1]}\n"
     ]
    }
   ],
   "execution_count": 36
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-23T14:42:34.668810Z",
     "start_time": "2024-09-23T14:42:34.665339Z"
    }
   },
   "source": [
    "collator = DataCollatorWithPadding(tokenizer=tokenizer)"
   ],
   "outputs": [],
   "execution_count": 37
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-23T14:42:37.123320Z",
     "start_time": "2024-09-23T14:42:37.119847Z"
    }
   },
   "source": [
    "from torch.utils.data import DataLoader"
   ],
   "outputs": [],
   "execution_count": 38
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-23T14:42:57.961750Z",
     "start_time": "2024-09-23T14:42:57.958278Z"
    }
   },
   "source": [
    "dl = DataLoader(tokenized_dataset, batch_size=4, collate_fn=collator, shuffle=True)"
   ],
   "outputs": [],
   "execution_count": 43
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-23T14:42:52.979565Z",
     "start_time": "2024-09-23T14:42:52.961710Z"
    }
   },
   "source": [
    "num = 0\n",
    "for batch in dl:\n",
    "    print(batch[\"input_ids\"].size())\n",
    "    num += 1\n",
    "    if num > 10:\n",
    "        break"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 128])\n",
      "torch.Size([4, 128])\n",
      "torch.Size([4, 128])\n",
      "torch.Size([4, 98])\n",
      "torch.Size([4, 85])\n",
      "torch.Size([4, 128])\n",
      "torch.Size([4, 128])\n",
      "torch.Size([4, 128])\n",
      "torch.Size([4, 128])\n",
      "torch.Size([4, 128])\n",
      "torch.Size([4, 70])\n"
     ]
    }
   ],
   "execution_count": 42
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:.conda-wdx_torch] *",
   "language": "python",
   "name": "conda-env-.conda-wdx_torch-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
